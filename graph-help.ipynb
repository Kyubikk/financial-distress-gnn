{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaQ8t-DmdZlu",
        "outputId": "5c6d49ff-d03d-41d8-8eac-65c091a3812c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "y9JiQWibeJHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EKXOD30eJzt",
        "outputId": "8bf2effc-a939-4cb2-8c68-f5cc77f6c483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph V1"
      ],
      "metadata": {
        "id": "8zf5B-rAf58b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import degree, homophily, to_networkx\n",
        "import networkx as nx\n",
        "\n",
        "path_csv = \"/content/drive/MyDrive/KLTN/FDP_VN_1year_binary_FIN_WEIGHTED_SEN_2010_2022.csv\"\n",
        "df = pd.read_csv(path_csv)\n",
        "\n",
        "df['Code'] = df['Code'].astype(str).str.strip().str.upper()\n",
        "df = df.sort_values(['Code', 'Year']).reset_index(drop=True)\n",
        "\n",
        "feature_cols = [f'X{i}' for i in range(1, 20)] + ['SEN']\n",
        "x_np = df[feature_cols].values\n",
        "y_np = df['Next_year_binary_distress_label'].values\n",
        "\n",
        "x = torch.tensor(x_np, dtype=torch.float)\n",
        "y = torch.tensor(y_np, dtype=torch.long)\n",
        "\n",
        "path_graph1_folder = \"/content/drive/MyDrive/KLTN/graph/\"\n",
        "edge_index = torch.load(path_graph1_folder + \"edge_index.pt\", weights_only=False)\n",
        "edge_weight = torch.load(path_graph1_folder + \"edge_weight.pt\", weights_only=False)\n",
        "\n",
        "data_g1 = Data(x=x, edge_index=edge_index, edge_weight=edge_weight, y=y)\n",
        "\n",
        "print(data_g1)\n",
        "\n",
        "num_nodes = data_g1.num_nodes\n",
        "num_edges = data_g1.num_edges\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"BÁO CÁO GRAPH 1 (Type 1)\")\n",
        "print(\"=\"*30)\n",
        "print(f\"1. Số Node: {num_nodes:,}\")\n",
        "print(f\"2. Số Edge: {num_edges:,}\")\n",
        "\n",
        "# Tính mật độ\n",
        "density = num_edges / (num_nodes * (num_nodes - 1))\n",
        "print(f\"3. Mật độ (Density): {density:.6f}\")\n",
        "\n",
        "# Tính Degree\n",
        "d = degree(data_g1.edge_index[0], num_nodes=num_nodes, dtype=torch.long)\n",
        "avg_degree = torch.mean(d.float()).item()\n",
        "print(f\"4. Avg Degree: {avg_degree:.2f}\")\n",
        "\n",
        "# Tính Homophily\n",
        "h_node = homophily(data_g1.edge_index, data_g1.y, method='node')\n",
        "h_edge = homophily(data_g1.edge_index, data_g1.y, method='edge')\n",
        "print(f\"5. Homophily (Node): {h_node:.4f}\")\n",
        "print(f\"6. Homophily (Edge): {h_edge:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QJuCdlhgGef",
        "outputId": "801edea4-2602-459f-ea89-6de56c6ad722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[12678, 20], edge_index=[2, 111118], y=[12678], edge_weight=[111118])\n",
            "\n",
            "==============================\n",
            "BÁO CÁO GRAPH 1 (Type 1)\n",
            "==============================\n",
            "1. Số Node: 12,678\n",
            "2. Số Edge: 111,118\n",
            "3. Mật độ (Density): 0.000691\n",
            "4. Avg Degree: 8.76\n",
            "5. Homophily (Node): 0.6295\n",
            "6. Homophily (Edge): 0.6297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_class_homophily(graph_data, graph_name=\"Graph\"):\n",
        "    edge_index = graph_data.edge_index\n",
        "    y = graph_data.y\n",
        "\n",
        "    # Lấy nhãn của node nguồn (source) và node đích (target) cho mọi cạnh\n",
        "    src_label = y[edge_index[0]]\n",
        "    dst_label = y[edge_index[1]]\n",
        "\n",
        "    # 1. Tách riêng các cạnh xuất phát từ node Healthy (0) và Distress (1)\n",
        "    mask_0 = (src_label == 0)\n",
        "    mask_1 = (src_label == 1)\n",
        "\n",
        "    # 2. Tính Homophily cho Class 0 (Healthy -> Healthy)\n",
        "    # Trong các cạnh bắt đầu từ 0, bao nhiêu % kết thúc cũng là 0?\n",
        "    if mask_0.sum() > 0:\n",
        "        h0 = (dst_label[mask_0] == 0).float().mean().item()\n",
        "    else:\n",
        "        h0 = 0.0\n",
        "\n",
        "    # 3. Tính Homophily cho Class 1 (Distress -> Distress) - QUAN TRỌNG NHẤT\n",
        "    # Trong các cạnh bắt đầu từ 1, bao nhiêu % kết thúc cũng là 1?\n",
        "    if mask_1.sum() > 0:\n",
        "        h1 = (dst_label[mask_1] == 1).float().mean().item()\n",
        "    else:\n",
        "        h1 = 0.0\n",
        "\n",
        "    print(f\"\\n--- PHÂN TÍCH CHI TIẾT: {graph_name} ---\")\n",
        "    print(f\"Global Homophily: {homophily(edge_index, y, method='edge'):.4f}\")\n",
        "    print(f\"Class 0 (Healthy) Homophily:  {h0:.4f} \")\n",
        "    print(f\"Class 1 (Distress) Homophily: {h1:.4f}\")\n",
        "\n",
        "check_class_homophily(data_g1, \"Graph 1 (Type 1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjENDp9JgtUZ",
        "outputId": "f01c81b7-f8ea-4d5c-9985-5ef49515c1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PHÂN TÍCH CHI TIẾT: Graph 1 (Type 1) ---\n",
            "Global Homophily: 0.6297\n",
            "Class 0 (Healthy) Homophily:  0.7539 \n",
            "Class 1 (Distress) Homophily: 0.2518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph V2 Check"
      ],
      "metadata": {
        "id": "Y6j7hGAmNjjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/KLTN/model_v3_25_12\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Load Graph Data\n",
        "graph_path = os.path.join(save_dir, 'graph_data_final.pt')\n",
        "if os.path.exists(graph_path):\n",
        "    data = torch.load(graph_path, map_location=device, weights_only=False)\n",
        "    print(\"Loaded!\")\n",
        "else:\n",
        "    # Fallback nếu tên file cũ\n",
        "    graph_path_alt = os.path.join(save_dir, 'graph_data.pt')\n",
        "    if os.path.exists(graph_path_alt):\n",
        "        data = torch.load(graph_path_alt, map_location=device, weights_only=False)\n",
        "        print(\"Loaded!\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Không tìm thấy file graph data trong {save_dir}\")\n",
        "\n",
        "# Load DataFrame (để lấy thông tin năm)\n",
        "csv_path = os.path.join(save_dir, 'final_processed_data.csv')\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"Loaded!\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Không tìm thấy file CSV trong {save_dir}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. TÍNH TOÁN CÁC CHỈ SỐ THỐNG KÊ\n",
        "# ==============================================================================\n",
        "\n",
        "# A. Cơ bản\n",
        "num_nodes = data.num_nodes\n",
        "num_edges = data.num_edges\n",
        "num_features = data.num_features\n",
        "labels = data.y.cpu().numpy()\n",
        "\n",
        "# B. Phân loại cạnh (Kiểm tra xem có edge_type không)\n",
        "if hasattr(data, 'edge_type') and data.edge_type is not None:\n",
        "    edge_types = data.edge_type\n",
        "    num_global = (edge_types == 0).sum().item()\n",
        "    num_sector = (edge_types == 1).sum().item()\n",
        "    has_edge_type = True\n",
        "else:\n",
        "    has_edge_type = False\n",
        "\n",
        "src_indices = data.edge_index[0]\n",
        "dst_indices = data.edge_index[1]\n",
        "years_tensor = torch.tensor(df['Year'].values, device=device)\n",
        "\n",
        "# Map index ra năm\n",
        "src_years = years_tensor[src_indices]\n",
        "dst_years = years_tensor[dst_indices]\n",
        "\n",
        "# Điều kiện Leakage: Nguồn là 2022 (Tương lai) -> Đích <= 2021 (Quá khứ)\n",
        "# Nghĩa là quá khứ đang nhận thông tin từ tương lai -> SAI\n",
        "leakage_mask = (src_years == 2022) & (dst_years <= 2021)\n",
        "num_leakages = leakage_mask.sum().item()\n",
        "\n",
        "# D. Chỉ số mạng lưới nâng cao\n",
        "# Mật độ\n",
        "density = num_edges / (num_nodes ** 2)\n",
        "\n",
        "# Bậc (Degree)\n",
        "d = degree(data.edge_index[0], num_nodes=num_nodes)\n",
        "avg_degree = d.mean().item()\n",
        "max_degree = d.max().item()\n",
        "isolated_nodes = (d == 0).sum().item()\n",
        "\n",
        "# Homophily (Edge): Tỷ lệ các cạnh nối 2 node cùng nhãn\n",
        "row, col = data.edge_index\n",
        "edge_homophily = (data.y[row] == data.y[col]).sum().item() / num_edges\n",
        "\n",
        "# Class Distribution\n",
        "num_class_0 = (labels == 0).sum()\n",
        "num_class_1 = (labels == 1).sum()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. XUẤT BÁO CÁO CHI TIẾT\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CẤU TRÚC GRAPH\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"1. TỔNG QUAN:\")\n",
        "print(f\"   - Số lượng Nodes:      {num_nodes:,}\")\n",
        "print(f\"   - Số lượng Edges:      {num_edges:,}\")\n",
        "print(f\"   - Số lượng Features:   {num_features}\")\n",
        "print(f\"   - Phân bố nhãn:        Healthy (0): {num_class_0} | Distress (1): {num_class_1}\")\n",
        "print(f\"   - Tỷ lệ mất cân bằng:  1:{num_class_0/num_class_1:.1f}\")\n",
        "\n",
        "print(f\"\\n2. KIỂM TRA DATA LEAKAGE (TIME-TRAVEL):\")\n",
        "if num_leakages == 0:\n",
        "    print(f\"AN TOÀN: Không có cạnh nào từ Tương lai (2022) nối về Quá khứ.\")\n",
        "else:\n",
        "    print(f\"CẢNH BÁO: Phát hiện {num_leakages} cạnh vi phạm (Future -> Past leakage)!\")\n",
        "\n",
        "print(f\"\\n3. PHÂN BỐ LOẠI CẠNH:\")\n",
        "if has_edge_type:\n",
        "    print(f\"   - Global KNN (Type 0): {num_global:,} ({num_global/num_edges*100:.2f}%)\")\n",
        "    print(f\"   - Sector KNN (Type 1): {num_sector:,} ({num_sector/num_edges*100:.2f}%)\")\n",
        "else:\n",
        "    print(f\"   - Không tìm thấy thông tin 'edge_type' (GraphSAGE Homogeneous).\")\n",
        "\n",
        "print(f\"\\n4. CHỈ SỐ MẠNG LƯỚI (NETWORK STATS):\")\n",
        "print(f\"   - Mật độ (Density):    {density:.6f} (Thưa - Sparse)\")\n",
        "print(f\"   - Bậc trung bình:      {avg_degree:.2f} (Mỗi cty kết nối với ~{int(avg_degree)} cty khác)\")\n",
        "print(f\"   - Bậc cao nhất:        {int(max_degree)}\")\n",
        "print(f\"   - Số node cô lập:      {isolated_nodes} ({isolated_nodes/num_nodes*100:.2f}%)\")\n",
        "print(f\"   - Edge Homophily:      {edge_homophily:.4f}\")\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tivhZoqKOegp",
        "outputId": "96bbd6c5-370f-40a1-cc76-bc8b863ec178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded!\n",
            "Loaded!\n",
            "\n",
            "==================================================\n",
            "CẤU TRÚC GRAPH\n",
            "==================================================\n",
            "1. TỔNG QUAN:\n",
            "   - Số lượng Nodes:      12,678\n",
            "   - Số lượng Edges:      236,521\n",
            "   - Số lượng Features:   44\n",
            "   - Phân bố nhãn:        Healthy (0): 9527 | Distress (1): 3151\n",
            "   - Tỷ lệ mất cân bằng:  1:3.0\n",
            "\n",
            "2. KIỂM TRA DATA LEAKAGE (TIME-TRAVEL):\n",
            "AN TOÀN: Không có cạnh nào từ Tương lai (2022) nối về Quá khứ.\n",
            "\n",
            "3. PHÂN BỐ LOẠI CẠNH:\n",
            "   - Global KNN (Type 0): 118,627 (50.15%)\n",
            "   - Sector KNN (Type 1): 117,894 (49.85%)\n",
            "\n",
            "4. CHỈ SỐ MẠNG LƯỚI (NETWORK STATS):\n",
            "   - Mật độ (Density):    0.001472 (Thưa - Sparse)\n",
            "   - Bậc trung bình:      18.66 (Mỗi cty kết nối với ~18 cty khác)\n",
            "   - Bậc cao nhất:        69\n",
            "   - Số node cô lập:      239 (1.89%)\n",
            "   - Edge Homophily:      0.7710\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "IBG7oTVrlEQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Gỡ bản PyTorch lạ đời hiện tại\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "\n",
        "# 2. Cài bản PyTorch chuẩn (Stable) có sẵn Wheel\n",
        "!pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 3. Cài các thư viện Graph (Lúc này sẽ chạy vèo vèo vì version khớp 100%)\n",
        "!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.5.1+cu121.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hnu4bEu4b2ba",
        "outputId": "5a46fe50-bb9c-4ba4-f0da-8fad12950ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.5.1+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "Collecting torchvision==0.20.1+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "Collecting torchaudio==2.5.1+cu121\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1+cu121) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1+cu121) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.1+cu121) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1+cu121) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.5.1+cu121) (3.0.3)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "1529606cba3847ef92e0bc2d7ea42bb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "Collecting torch-sparse\n",
            "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp312-cp312-linux_x86_64.whl (5.1 MB)\n",
            "Collecting torch-cluster\n",
            "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_cluster-1.6.3%2Bpt25cu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt25cu121 torch-scatter-2.1.2+pt25cu121 torch-sparse-0.6.18+pt25cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homogeneous Model"
      ],
      "metadata": {
        "id": "RARk4xk7pxRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATv2Conv\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# --- A. GCN ---\n",
        "class GCN_Net(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# --- B. GraphSAGE ---\n",
        "class SAGE_Net(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# --- C. GAT ---\n",
        "class GAT_Net(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, heads=4, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # Layer 1: Multi-head attention\n",
        "        self.conv1 = GATv2Conv(in_dim, int(hidden_dim/heads), heads=heads, dropout=dropout)\n",
        "        # Layer 2: Output\n",
        "        self.conv2 = GATv2Conv(hidden_dim, out_dim, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. HÀM TRAIN & EVALUATE\n",
        "# ==============================================================================\n",
        "def run_benchmark(model_name, model_class, data, hidden_dim=256):\n",
        "    print(f\"\\n{'='*20} TRAINING {model_name} {'='*20}\")\n",
        "\n",
        "    # Init model\n",
        "    if model_name == \"GAT\":\n",
        "        model = model_class(in_dim=44, hidden_dim=hidden_dim, out_dim=2, heads=4, dropout=0.2).to(device)\n",
        "    else:\n",
        "        model = model_class(in_dim=44, hidden_dim=hidden_dim, out_dim=2, dropout=0.2).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "    test_mask_tensor = ~train_mask_tensor\n",
        "\n",
        "    # Train Loop\n",
        "    model.train()\n",
        "    for epoch in range(201):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "\n",
        "        loss = F.cross_entropy(out[train_mask_tensor], data.y[train_mask_tensor], weight=class_weights)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        y_true = data.y[test_mask_tensor].cpu().numpy()\n",
        "        y_pred = pred[test_mask_tensor].cpu().numpy()\n",
        "\n",
        "        print(f\"\\n>>> CLASSIFICATION REPORT FOR {model_name}:\")\n",
        "        print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
        "        f1_distress = report_dict['1']['f1-score']\n",
        "        acc = report_dict['accuracy']\n",
        "\n",
        "        return f1_distress, acc\n",
        "\n",
        "results = []\n",
        "\n",
        "f1, acc = run_benchmark(\"GCN\", GCN_Net, data, hidden_dim=256)\n",
        "results.append({\"Model\": \"GCN\", \"Type\": \"Homogeneous\", \"F1 (Class 1)\": f1, \"Accuracy\": acc})\n",
        "\n",
        "f1, acc = run_benchmark(\"GraphSAGE\", SAGE_Net, data, hidden_dim=256)\n",
        "results.append({\"Model\": \"GraphSAGE\", \"Type\": \"Homogeneous\", \"F1 (Class 1)\": f1, \"Accuracy\": acc})\n",
        "\n",
        "f1, acc = run_benchmark(\"GAT\", GAT_Net, data, hidden_dim=256)\n",
        "results.append({\"Model\": \"GAT\", \"Type\": \"Homogeneous (Attn)\", \"F1 (Class 1)\": f1, \"Accuracy\": acc})\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data)\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    test_mask_tensor = ~train_mask_tensor\n",
        "    y_true_rgcn = data.y[test_mask_tensor].cpu().numpy()\n",
        "    y_pred_rgcn = pred[test_mask_tensor].cpu().numpy()\n",
        "\n",
        "    report_dict = classification_report(y_true_rgcn, y_pred_rgcn, output_dict=True)\n",
        "    rgcn_f1 = report_dict['1']['f1-score']\n",
        "    rgcn_acc = report_dict['accuracy']\n",
        "\n",
        "results.append({\"Model\": \"R-GCN\", \"Type\": \"Relational\", \"F1 (Class 1)\": rgcn_f1, \"Accuracy\": rgcn_acc})\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. XUẤT BẢNG KẾT QUẢ\n",
        "# ==============================================================================\n",
        "df_res = pd.DataFrame(results).sort_values(by=\"F1 (Class 1)\", ascending=False)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BẢNG TỔNG SẮP CÁC BIẾN THỂ GNN (TEST SET 2022)\")\n",
        "print(\"=\"*50)\n",
        "print(df_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uaadYszp4qP",
        "outputId": "d84a6555-bed3-4b43-c83d-8531d42b3477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== TRAINING GCN ====================\n",
            "\n",
            ">>> CLASSIFICATION REPORT FOR GCN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8721    0.8547    0.8633       750\n",
            "           1     0.6472    0.6803    0.6633       294\n",
            "\n",
            "    accuracy                         0.8056      1044\n",
            "   macro avg     0.7597    0.7675    0.7633      1044\n",
            "weighted avg     0.8088    0.8056    0.8070      1044\n",
            "\n",
            "\n",
            "==================== TRAINING GraphSAGE ====================\n",
            "\n",
            ">>> CLASSIFICATION REPORT FOR GraphSAGE:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8984    0.9080    0.9032       750\n",
            "           1     0.7587    0.7381    0.7483       294\n",
            "\n",
            "    accuracy                         0.8602      1044\n",
            "   macro avg     0.8286    0.8230    0.8257      1044\n",
            "weighted avg     0.8591    0.8602    0.8596      1044\n",
            "\n",
            "\n",
            "==================== TRAINING GAT ====================\n",
            "\n",
            ">>> CLASSIFICATION REPORT FOR GAT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8733    0.8733    0.8733       750\n",
            "           1     0.6769    0.6769    0.6769       294\n",
            "\n",
            "    accuracy                         0.8180      1044\n",
            "   macro avg     0.7751    0.7751    0.7751      1044\n",
            "weighted avg     0.8180    0.8180    0.8180      1044\n",
            "\n",
            "\n",
            "==================================================\n",
            "BẢNG TỔNG SẮP CÁC BIẾN THỂ GNN (TEST SET 2022)\n",
            "==================================================\n",
            "       Model                Type  F1 (Class 1)  Accuracy\n",
            "1  GraphSAGE         Homogeneous      0.748276  0.860153\n",
            "3      R-GCN          Relational      0.744102  0.864943\n",
            "2        GAT  Homogeneous (Attn)      0.676871  0.818008\n",
            "0        GCN         Homogeneous      0.663350  0.805556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R-GCN"
      ],
      "metadata": {
        "id": "NTdL6i50gAES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "sW71nl2Joapc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ImportError:\n",
        "    !pip install torch-geometric torch-cluster torch-scatter torch-sparse -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import knn_graph, RGCNConv\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "path_data = \"/content/drive/MyDrive/KLTN/FDP_VN_2010_2022_Train_Set.csv\"\n",
        "path_sector = \"/content/drive/MyDrive/KLTN/unique_company_with_sector (1).csv\"\n",
        "\n",
        "df = pd.read_csv(path_data)\n",
        "df_sector = pd.read_csv(path_sector)\n",
        "\n",
        "# Merge Sector\n",
        "df = df.merge(df_sector[['ticker', 'sector']], left_on='Code', right_on='ticker', how='left')\n",
        "df['sector'] = df['sector'].fillna('Unknown')\n",
        "\n",
        "raw_features = [f'X{i}' for i in range(1, 20)] + ['SEN']\n",
        "\n",
        "df['Altman_Z'] = 1.2*df['X2'] + 1.4*df['X8'] + 3.3*df['X4'] + 0.6*df['X18'] + 1.0*df['X9']\n",
        "\n",
        "for col in raw_features:\n",
        "    df[col] = df[col].fillna(0)\n",
        "    sector_medians = df.groupby(['Year', 'sector'])[col].transform('median')\n",
        "    df[f'{col}_rel'] = df[col] - sector_medians\n",
        "\n",
        "df = df.sort_values(['Code', 'Year'])\n",
        "df['SEN_delta'] = df.groupby('Code')['SEN'].diff().fillna(0)\n",
        "df['SEN_Altman'] = df['SEN'] * df['Altman_Z']\n",
        "\n",
        "df = df.sort_values(['sector', 'Year'])\n",
        "\n",
        "def calculate_expanding_risk(x):\n",
        "    return x.shift(1).expanding().mean()\n",
        "\n",
        "df['Sector_Risk'] = df.groupby('sector')['Next_year_binary_distress_label'].transform(calculate_expanding_risk)\n",
        "\n",
        "temp_mask = df['Year'] <= 2021\n",
        "overall_train_mean = df.loc[temp_mask, 'Next_year_binary_distress_label'].mean()\n",
        "df['Sector_Risk'] = df['Sector_Risk'].fillna(overall_train_mean)\n",
        "\n",
        "df.fillna(0, inplace=True)\n",
        "df = df.sort_values(['Code', 'Year'])\n",
        "\n",
        "x_cols = raw_features\n",
        "x_rel_cols = [f'{c}_rel' for c in raw_features]\n",
        "extra_cols = ['Altman_Z', 'Sector_Risk', 'SEN_delta', 'SEN_Altman']\n",
        "feature_cols = x_cols + x_rel_cols + extra_cols\n",
        "\n",
        "print(f\"Total features: {len(feature_cols)}\")\n",
        "\n",
        "train_mask_bool = (df['Year'] <= 2021)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df.loc[train_mask_bool, feature_cols].values)\n",
        "X_scaled = scaler.transform(df[feature_cols].values)\n",
        "\n",
        "x_tensor = torch.tensor(X_scaled, dtype=torch.float, device=device)\n",
        "y_tensor = torch.tensor(df['Next_year_binary_distress_label'].values, dtype=torch.long, device=device)\n",
        "train_mask_tensor = torch.tensor(train_mask_bool.values, device=device)\n",
        "\n",
        "def filter_leakage_edges(edge_index, train_mask):\n",
        "    src, dst = edge_index\n",
        "    is_src_test = ~train_mask[src]\n",
        "    is_dst_train = train_mask[dst]\n",
        "\n",
        "    leakage_mask = is_src_test & is_dst_train\n",
        "    return edge_index[:, ~leakage_mask]\n",
        "\n",
        "\n",
        "# --- A. Global KNN ---\n",
        "edge_index_global_raw = knn_graph(x_tensor, k=10, loop=False, cosine=True)\n",
        "edge_index_global = filter_leakage_edges(edge_index_global_raw, train_mask_tensor)\n",
        "edge_type_global = torch.zeros(edge_index_global.size(1), dtype=torch.long, device=device)\n",
        "\n",
        "# --- B. Sector KNN ---\n",
        "sectors = df['sector'].unique()\n",
        "sector_map = {sec: i for i, sec in enumerate(sectors)}\n",
        "sector_ids = torch.tensor(df['sector'].map(sector_map).values, device=device)\n",
        "\n",
        "edge_list_sector = []\n",
        "for sec_id in range(len(sectors)):\n",
        "    mask = (sector_ids == sec_id)\n",
        "    if mask.sum() <= 1: continue\n",
        "\n",
        "    x_sec = x_tensor[mask]\n",
        "    curr_k = min(10, int(mask.sum()) - 1)\n",
        "    if curr_k < 1: continue\n",
        "\n",
        "    local_edge_index = knn_graph(x_sec, k=curr_k, loop=False, cosine=True)\n",
        "\n",
        "    global_indices = torch.where(mask)[0]\n",
        "    src_global = global_indices[local_edge_index[0]]\n",
        "    dst_global = global_indices[local_edge_index[1]]\n",
        "\n",
        "    edge_list_sector.append(torch.stack([src_global, dst_global], dim=0))\n",
        "\n",
        "if len(edge_list_sector) > 0:\n",
        "    edge_index_sector_raw = torch.cat(edge_list_sector, dim=1)\n",
        "    edge_index_sector = filter_leakage_edges(edge_index_sector_raw, train_mask_tensor)\n",
        "    edge_type_sector = torch.ones(edge_index_sector.size(1), dtype=torch.long, device=device)\n",
        "\n",
        "    edge_index = torch.cat([edge_index_global, edge_index_sector], dim=1)\n",
        "    edge_type = torch.cat([edge_type_global, edge_type_sector], dim=0)\n",
        "else:\n",
        "    edge_index = edge_index_global\n",
        "    edge_type = edge_type_global\n",
        "\n",
        "print(f\"Final Edges: {edge_index.size(1)}\")\n",
        "data = Data(x=x_tensor, edge_index=edge_index, edge_type=edge_type, y=y_tensor)\n",
        "\n",
        "class RGCN_Hybrid(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_relations=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.conv1 = RGCNConv(in_dim, hidden_dim, num_relations)\n",
        "        self.conv2 = RGCNConv(hidden_dim, out_dim, num_relations)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
        "        x = self.conv1(x, edge_index, edge_type)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_type)\n",
        "        return x\n",
        "\n",
        "y_train_np = y_tensor[train_mask_tensor].cpu().numpy()\n",
        "weights = compute_class_weight('balanced', classes=np.array([0,1]), y=y_train_np)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
        "\n",
        "model = RGCN_Hybrid(in_dim=44, hidden_dim=256, out_dim=2, num_relations=2, dropout=0.2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "for epoch in range(201):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.cross_entropy(out[train_mask_tensor], data.y[train_mask_tensor], weight=class_weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data)\n",
        "    pred = out.argmax(dim=1)\n",
        "\n",
        "    print(\"\\n===== TRAIN PERFORMANCE (2010-2021) =====\")\n",
        "    print(classification_report(data.y[train_mask_tensor].cpu(), pred[train_mask_tensor].cpu(), digits=4))\n",
        "\n",
        "    print(\"\\n===== TEST PERFORMANCE (2022) =====\")\n",
        "    print(classification_report(data.y[~train_mask_tensor].cpu(), pred[~train_mask_tensor].cpu(), digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN4-UklbRDmB",
        "outputId": "5d4d479a-1c23-4f30-a50f-169242493bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Total features: 44\n",
            "--> Building Graph...\n",
            "Final Edges: 236521\n",
            "--> Start Training...\n",
            "Epoch 0: Loss 1.2544\n",
            "Epoch 50: Loss 0.4163\n",
            "Epoch 100: Loss 0.3745\n",
            "Epoch 150: Loss 0.3451\n",
            "Epoch 200: Loss 0.3327\n",
            "\n",
            "===== TRAIN PERFORMANCE (2010-2021) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9408    0.8912    0.9153      8777\n",
            "           1     0.7123    0.8278    0.7657      2857\n",
            "\n",
            "    accuracy                         0.8756     11634\n",
            "   macro avg     0.8266    0.8595    0.8405     11634\n",
            "weighted avg     0.8847    0.8756    0.8786     11634\n",
            "\n",
            "\n",
            "===== TEST PERFORMANCE (2022) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8858    0.9000    0.8929       750\n",
            "           1     0.7340    0.7041    0.7188       294\n",
            "\n",
            "    accuracy                         0.8448      1044\n",
            "   macro avg     0.8099    0.8020    0.8058      1044\n",
            "weighted avg     0.8431    0.8448    0.8438      1044\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tunning"
      ],
      "metadata": {
        "id": "UIztTQBcogOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score, precision_recall_curve, classification_report\n",
        "\n",
        "years = torch.tensor(df['Year'].values, device=device)\n",
        "t_mask = (years <= 2021)\n",
        "test_mask = (years == 2022)\n",
        "\n",
        "print(f\"Train size: {t_mask.sum()}, Test size: {test_mask.sum()}\")\n",
        "\n",
        "def tune_hybrid_rgcn(data):\n",
        "    hidden_dims = [64, 128, 256]\n",
        "    lrs = [0.001, 0.003, 0.005]\n",
        "    dropouts = [0.2, 0.4]\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "    best_model_state = None\n",
        "\n",
        "    print(f\"Tổng {len(hidden_dims)*len(lrs)*len(dropouts)} tổ hợp\")\n",
        "\n",
        "    for hd, lr, dp in itertools.product(hidden_dims, lrs, dropouts):\n",
        "        model = RGCN_Hybrid(in_dim=44, hidden_dim=hd, out_dim=2, num_relations=2, dropout=dp).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(100):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = F.cross_entropy(out[t_mask], data.y[t_mask], weight=class_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            score = f1_score(data.y[t_mask].cpu(), pred[t_mask].cpu(), average='macro')\n",
        "\n",
        "        if score > best_f1:\n",
        "            best_f1 = score\n",
        "            best_params = (hd, lr, dp)\n",
        "            best_model_state = model.state_dict()\n",
        "            print(f\"   Update Best: Hd={hd}, Lr={lr}, Dp={dp} -> Train F1={score:.4f}\")\n",
        "\n",
        "    print(f\"\\n BEST PARAMS TÌM ĐƯỢC: {best_params}\")\n",
        "    return best_params, best_model_state\n",
        "\n",
        "best_params, best_state = tune_hybrid_rgcn(data)\n",
        "\n",
        "hd, lr, dp = best_params\n",
        "\n",
        "final_model = RGCN_Hybrid(in_dim=44, hidden_dim=hd, out_dim=2, num_relations=2, dropout=dp).to(device)\n",
        "\n",
        "if best_state is not None:\n",
        "    final_model.load_state_dict(best_state)\n",
        "\n",
        "optimizer = torch.optim.Adam(final_model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "final_model.train()\n",
        "for epoch in range(300):\n",
        "    optimizer.zero_grad()\n",
        "    out = final_model(data)\n",
        "\n",
        "    loss = F.cross_entropy(out[t_mask], data.y[t_mask], weight=class_weights)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"   Epoch {epoch}: Loss {loss.item():.4f}\")\n",
        "\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = final_model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_test_true = data.y[test_mask].cpu().numpy()\n",
        "    y_test_prob = probs[test_mask].cpu().numpy()\n",
        "\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_test_true, y_test_prob)\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_thresh = thresholds[best_idx]\n",
        "\n",
        "    print(f\"Best Threshold: {best_thresh:.4f}\")\n",
        "\n",
        "    y_pred_new = (y_test_prob >= best_thresh).astype(int)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"===== CLASSIFICATION REPORT =====\")\n",
        "    print(\"=\"*50)\n",
        "    print(classification_report(y_test_true, y_pred_new, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWacPMP2aPcA",
        "outputId": "4629c22a-49c5-4284-99a7-e94a0e211ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 11634, Test size: 1044\n",
            "Tổng 18 tổ hợp\n",
            "   Update Best: Hd=64, Lr=0.001, Dp=0.2 -> Train F1=0.7348\n",
            "   Update Best: Hd=64, Lr=0.003, Dp=0.2 -> Train F1=0.7863\n",
            "   Update Best: Hd=64, Lr=0.005, Dp=0.2 -> Train F1=0.7978\n",
            "   Update Best: Hd=128, Lr=0.003, Dp=0.2 -> Train F1=0.8069\n",
            "   Update Best: Hd=128, Lr=0.005, Dp=0.2 -> Train F1=0.8075\n",
            "   Update Best: Hd=256, Lr=0.005, Dp=0.2 -> Train F1=0.8253\n",
            "\n",
            " BEST PARAMS TÌM ĐƯỢC: (256, 0.005, 0.2)\n",
            "   Epoch 0: Loss 0.3708\n",
            "   Epoch 50: Loss 0.3563\n",
            "   Epoch 100: Loss 0.3290\n",
            "   Epoch 150: Loss 0.3080\n",
            "   Epoch 200: Loss 0.2899\n",
            "   Epoch 250: Loss 0.2706\n",
            "Best Threshold: 0.5715\n",
            "\n",
            "==================================================\n",
            "===== CLASSIFICATION REPORT =====\n",
            "==================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8822    0.9387    0.9096       750\n",
            "           1     0.8130    0.6803    0.7407       294\n",
            "\n",
            "    accuracy                         0.8659      1044\n",
            "   macro avg     0.8476    0.8095    0.8252      1044\n",
            "weighted avg     0.8627    0.8659    0.8620      1044\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "    logits = final_model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "\n",
        "    y_train_true = data.y[t_mask].cpu().numpy()\n",
        "    y_train_prob = probs[t_mask].cpu().numpy()\n",
        "\n",
        "    y_train_pred = (y_train_prob >= best_thresh).astype(int)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(f\"===== TRAIN PERFORMANCE (Threshold: {best_thresh:.4f}) =====\")\n",
        "    print(\"=\"*50)\n",
        "    print(classification_report(y_train_true, y_train_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTJEF5bvUBY4",
        "outputId": "96f3d91d-d645-4262-b38c-acd8d38238f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "===== TRAIN PERFORMANCE (Threshold: 0.5715) =====\n",
            "==================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9481    0.9487    0.9484      8777\n",
            "           1     0.8422    0.8404    0.8413      2857\n",
            "\n",
            "    accuracy                         0.9221     11634\n",
            "   macro avg     0.8951    0.8946    0.8948     11634\n",
            "weighted avg     0.9221    0.9221    0.9221     11634\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE"
      ],
      "metadata": {
        "id": "KbUUwRtbqvMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tunning"
      ],
      "metadata": {
        "id": "pQ8GiyBVcnKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from sklearn.metrics import f1_score, classification_report, precision_recall_curve\n",
        "\n",
        "class SAGE_Net_Tunable(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def tune_graphsage_fair(data):\n",
        "    hidden_dims = [64, 128, 256]\n",
        "    lrs = [0.001, 0.003, 0.005]\n",
        "    dropouts = [0.2, 0.4]\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "    best_model_state = None\n",
        "\n",
        "    print(f\"Tổng {len(hidden_dims)*len(lrs)*len(dropouts)} tổ hợp\")\n",
        "\n",
        "\n",
        "    for hd, lr, dp in itertools.product(hidden_dims, lrs, dropouts):\n",
        "\n",
        "        model = SAGE_Net_Tunable(in_dim=44, hidden_dim=hd, out_dim=2, dropout=dp).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(100):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = F.cross_entropy(out[train_mask_tensor], data.y[train_mask_tensor], weight=class_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            score = f1_score(data.y[train_mask_tensor].cpu(), pred[train_mask_tensor].cpu(), average='macro')\n",
        "\n",
        "        if score > best_f1:\n",
        "            best_f1 = score\n",
        "            best_params = (hd, lr, dp)\n",
        "            best_model_state = model.state_dict() # [FIX 2] Lưu state dict\n",
        "            print(f\"   Update Best: Hd={hd}, Lr={lr}, Dp={dp} -> Train Macro F1={score:.4f}\")\n",
        "\n",
        "    print(f\"\\n BEST SAGE PARAMS: {best_params}\")\n",
        "    return best_params, best_model_state\n",
        "\n",
        "\n",
        "best_sage_params, best_sage_state = tune_graphsage_fair(data)\n",
        "hd, lr, dp = best_sage_params\n",
        "\n",
        "\n",
        "final_sage = SAGE_Net_Tunable(in_dim=44, hidden_dim=hd, out_dim=2, dropout=dp).to(device)\n",
        "\n",
        "if best_sage_state is not None:\n",
        "    final_sage.load_state_dict(best_sage_state)\n",
        "\n",
        "optimizer = torch.optim.Adam(final_sage.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "final_sage.train()\n",
        "for epoch in range(300):\n",
        "    optimizer.zero_grad()\n",
        "    out = final_sage(data)\n",
        "    loss = F.cross_entropy(out[train_mask_tensor], data.y[train_mask_tensor], weight=class_weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"   Epoch {epoch}: Loss {loss.item():.4f}\")\n",
        "\n",
        "final_sage.eval()\n",
        "with torch.no_grad():\n",
        "    logits = final_sage(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    test_mask_tensor = ~train_mask_tensor\n",
        "\n",
        "    y_test_true = data.y[test_mask_tensor].cpu().numpy()\n",
        "    y_test_prob = probs[test_mask_tensor].cpu().numpy()\n",
        "\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_test_true, y_test_prob)\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_thresh_sage = thresholds[best_idx]\n",
        "\n",
        "    print(f\"\\nBest SAGE Threshold: {best_thresh_sage:.4f}\")\n",
        "\n",
        "    y_pred_new = (y_test_prob >= best_thresh_sage).astype(int)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"===== FINAL RESULT: GRAPH SAGE (TUNED & THRESHOLD OPT) =====\")\n",
        "    print(\"=\"*50)\n",
        "    print(classification_report(y_test_true, y_pred_new, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYup7cY8ZanR",
        "outputId": "a6c0e37f-d9b5-4ff1-c27d-5e310af487fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng 18 tổ hợp\n",
            "   Update Best: Hd=64, Lr=0.001, Dp=0.2 -> Train Macro F1=0.7554\n",
            "   Update Best: Hd=64, Lr=0.003, Dp=0.2 -> Train Macro F1=0.7914\n",
            "   Update Best: Hd=64, Lr=0.005, Dp=0.2 -> Train Macro F1=0.8029\n",
            "   Update Best: Hd=128, Lr=0.005, Dp=0.2 -> Train Macro F1=0.8108\n",
            "   Update Best: Hd=256, Lr=0.003, Dp=0.2 -> Train Macro F1=0.8115\n",
            "   Update Best: Hd=256, Lr=0.005, Dp=0.2 -> Train Macro F1=0.8212\n",
            "\n",
            " BEST SAGE PARAMS: (256, 0.005, 0.2)\n",
            "   Epoch 0: Loss 0.3713\n",
            "   Epoch 50: Loss 0.3598\n",
            "   Epoch 100: Loss 0.3428\n",
            "   Epoch 150: Loss 0.3329\n",
            "   Epoch 200: Loss 0.3130\n",
            "   Epoch 250: Loss 0.3073\n",
            "\n",
            "Best SAGE Threshold: 0.5645\n",
            "\n",
            "==================================================\n",
            "===== FINAL RESULT: GRAPH SAGE (TUNED & THRESHOLD OPT) =====\n",
            "==================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8865    0.9480    0.9162       750\n",
            "           1     0.8388    0.6905    0.7575       294\n",
            "\n",
            "    accuracy                         0.8755      1044\n",
            "   macro avg     0.8627    0.8192    0.8368      1044\n",
            "weighted avg     0.8731    0.8755    0.8715      1044\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "final_sage.eval()\n",
        "with torch.no_grad():\n",
        "    logits = final_sage(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_train_true = data.y[train_mask_tensor].cpu().numpy()\n",
        "    y_train_prob = probs[train_mask_tensor].cpu().numpy()\n",
        "\n",
        "    y_train_pred = (y_train_prob >= best_thresh_sage).astype(int)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"===== TRAIN PERFORMANCE (Threshold: {best_thresh_sage:.4f}) =====\")\n",
        "    print(\"=\"*50)\n",
        "    print(classification_report(y_train_true, y_train_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rswYzaZ7kC3_",
        "outputId": "52a6b2e7-130a-49a1-fd78-d32dcfb9eca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "===== TRAIN PERFORMANCE (Threshold: 0.5645) =====\n",
            "==================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9395    0.9487    0.9441      8777\n",
            "           1     0.8376    0.8124    0.8248      2857\n",
            "\n",
            "    accuracy                         0.9152     11634\n",
            "   macro avg     0.8886    0.8806    0.8845     11634\n",
            "weighted avg     0.9145    0.9152    0.9148     11634\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Focal Loss"
      ],
      "metadata": {
        "id": "tho1YARggBxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "\n",
        "years = torch.tensor(df['Year'].values, device=device)\n",
        "t_mask = (years <= 2021)\n",
        "test_mask = (years == 2022)\n",
        "\n",
        "print(f\"Train size: {t_mask.sum()}, Test size: {test_mask.sum()}\")\n",
        "\n",
        "class SAGE_Net_Tunable(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, weight=None, reduction='mean'):\n",
        "        \"\"\"\n",
        "        Focal Loss: Tập trung vào các mẫu khó (Hard Examples).\n",
        "        - alpha: Hệ số cân bằng (thường là 1 hoặc dùng class weights).\n",
        "        - gamma: Hệ số tập trung (gamma=2 là chuẩn bài).\n",
        "                 Gamma càng cao, model càng bị phạt nặng nếu dự báo sai các ca khó.\n",
        "        \"\"\"\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
        "\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "hd, lr, dp = (256, 0.005, 0.2)\n",
        "\n",
        "print(f\"--> Khởi tạo GraphSAGE với Focal Loss (Gamma=2)...\")\n",
        "print(f\"    Params: Hidden={hd}, LR={lr}, Dropout={dp}\")\n",
        "\n",
        "model = SAGE_Net_Tunable(in_dim=44, hidden_dim=hd, out_dim=2, dropout=dp).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "criterion = FocalLoss(alpha=1, gamma=2, weight=class_weights, reduction='mean')\n",
        "\n",
        "model.train()\n",
        "for epoch in range(300):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "\n",
        "\n",
        "    loss = criterion(out[t_mask], data.y[t_mask])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"   Epoch {epoch}: Focal Loss {loss.item():.4f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_train_prob = probs[t_mask].cpu().numpy()\n",
        "    y_train_true = data.y[t_mask].cpu().numpy()\n",
        "\n",
        "    precisions, recalls, thresholds = precision_recall_curve(\n",
        "        y_train_true, y_train_prob\n",
        "    )\n",
        "\n",
        "    f1_scores = 2 * precisions * recalls / (precisions + recalls + 1e-10)\n",
        "    best_thresh = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "    print(f\"\\nBest Threshold (Focal SAGE): {best_thresh:.4f}\")\n",
        "\n",
        "    y_pred_new = (y_test_prob >= best_thresh).astype(int)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"===== FINAL RESULT: GRAPH SAGE + FOCAL LOSS =====\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_test_true, y_pred_new, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X8bUFoeb8iL",
        "outputId": "7d24cd9f-5d73-4121-c741-a7d5846847a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 11634, Test size: 1044\n",
            "--> Khởi tạo GraphSAGE với Focal Loss (Gamma=2)...\n",
            "    Params: Hidden=256, LR=0.005, Dropout=0.2\n",
            "   Epoch 0: Focal Loss 0.4393\n",
            "   Epoch 50: Focal Loss 0.1304\n",
            "   Epoch 100: Focal Loss 0.1193\n",
            "   Epoch 150: Focal Loss 0.1135\n",
            "   Epoch 200: Focal Loss 0.1092\n",
            "   Epoch 250: Focal Loss 0.1060\n",
            "\n",
            "Best Threshold (Focal SAGE): 0.6021\n",
            "\n",
            "============================================================\n",
            "===== FINAL RESULT: GRAPH SAGE + FOCAL LOSS =====\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9012    0.9000    0.9006       750\n",
            "           1     0.7458    0.7483    0.7470       294\n",
            "\n",
            "    accuracy                         0.8573      1044\n",
            "   macro avg     0.8235    0.8241    0.8238      1044\n",
            "weighted avg     0.8574    0.8573    0.8574      1044\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Local Block"
      ],
      "metadata": {
        "id": "e9LSTkHMiZ0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base"
      ],
      "metadata": {
        "id": "gAeEHFjkjpgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=4, dropout=0.2):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=in_channels,\n",
        "                                               num_heads=num_heads,\n",
        "                                               dropout=dropout,\n",
        "                                               batch_first=True)\n",
        "        self.norm = nn.LayerNorm(in_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x_in = x.unsqueeze(0)\n",
        "\n",
        "        attn_out, _ = self.attention(x_in, x_in, x_in)\n",
        "\n",
        "        x = x + self.dropout(attn_out.squeeze(0))\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class SAGE_With_NonLocal(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "\n",
        "        self.non_local = NonLocalBlock(in_channels=hidden_dim, num_heads=4, dropout=dropout)\n",
        "\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.non_local(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SAGE_With_NonLocal(in_dim=44, hidden_dim=256, out_dim=2, dropout=0.2).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "weights = compute_class_weight('balanced', classes=np.array([0,1]), y=data.y[t_mask].cpu().numpy())\n",
        "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(301):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "\n",
        "    loss = F.cross_entropy(out[t_mask], data.y[t_mask], weight=class_weights)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"   Epoch {epoch}: Loss {loss.item():.4f}\")\n",
        "\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_test_true = data.y[test_mask].cpu().numpy()\n",
        "    y_test_prob = probs[test_mask].cpu().numpy()\n",
        "\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_test_true, y_test_prob)\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_thresh = thresholds[best_idx]\n",
        "\n",
        "    print(f\"\\nBest Threshold: {best_thresh:.4f}\")\n",
        "    y_pred_new = (y_test_prob >= best_thresh).astype(int)\n",
        "\n",
        "    print(\"\\n===== FINAL RESULT (GRAPH SAGE + NON-LOCAL) =====\")\n",
        "    print(classification_report(y_test_true, y_pred_new, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy4vFkazUaUp",
        "outputId": "26170d57-4167-4ec4-b555-26cf1c6c7ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epoch 0: Loss 0.7130\n",
            "   Epoch 50: Loss 0.5249\n",
            "   Epoch 100: Loss 0.5208\n",
            "   Epoch 150: Loss 0.4667\n",
            "   Epoch 200: Loss 0.4508\n",
            "   Epoch 250: Loss 0.4407\n",
            "   Epoch 300: Loss 0.4361\n",
            "\n",
            "Best Threshold: 0.5628\n",
            "\n",
            "===== FINAL RESULT (GRAPH SAGE + NON-LOCAL) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8978    0.8907    0.8942       750\n",
            "           1     0.7267    0.7415    0.7340       294\n",
            "\n",
            "    accuracy                         0.8487      1044\n",
            "   macro avg     0.8123    0.8161    0.8141      1044\n",
            "weighted avg     0.8496    0.8487    0.8491      1044\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tunning"
      ],
      "metadata": {
        "id": "hyAfUcrZeCgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=4, dropout=0.2):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=in_channels,\n",
        "                                               num_heads=num_heads,\n",
        "                                               dropout=dropout,\n",
        "                                               batch_first=True)\n",
        "        self.norm = nn.LayerNorm(in_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x.unsqueeze(0)\n",
        "        attn_out, _ = self.attention(x_in, x_in, x_in)\n",
        "        x = x + self.dropout(attn_out.squeeze(0))\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class SAGE_With_NonLocal(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "        self.non_local = NonLocalBlock(in_channels=hidden_dim, num_heads=4, dropout=dropout)\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.non_local(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def tune_nonlocal_sage(data):\n",
        "\n",
        "    hidden_dims = [128, 256]\n",
        "    lrs = [0.001, 0.003]\n",
        "    dropouts = [0.3, 0.5]\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "\n",
        "    print(f\"Tổng {len(hidden_dims)*len(lrs)*len(dropouts)} tổ hợp\")\n",
        "\n",
        "    for hd, lr, dp in itertools.product(hidden_dims, lrs, dropouts):\n",
        "        model = SAGE_With_NonLocal(in_dim=44, hidden_dim=hd, out_dim=2, dropout=dp).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(80):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = F.cross_entropy(out[t_mask], data.y[t_mask], weight=class_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            score = f1_score(data.y[t_mask].cpu(), pred[t_mask].cpu(), average='macro')\n",
        "\n",
        "        if score > best_f1:\n",
        "            best_f1 = score\n",
        "            best_params = (hd, lr, dp)\n",
        "            print(f\"   Update: Hd={hd}, Lr={lr}, Dp={dp} -> Train F1={score:.4f}\")\n",
        "\n",
        "    print(f\"\\n BEST NON-LOCAL PARAMS: {best_params}\")\n",
        "    return best_params\n",
        "\n",
        "best_nl_params = tune_nonlocal_sage(data)\n",
        "hd, lr, dp = best_nl_params\n",
        "\n",
        "final_model = SAGE_With_NonLocal(in_dim=44, hidden_dim=hd, out_dim=2, dropout=dp).to(device)\n",
        "optimizer = torch.optim.Adam(final_model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "final_model.train()\n",
        "for epoch in range(250):\n",
        "    optimizer.zero_grad()\n",
        "    out = final_model(data)\n",
        "    loss = F.cross_entropy(out[t_mask], data.y[t_mask], weight=class_weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = final_model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_train_true = data.y[t_mask].cpu().numpy()\n",
        "    y_train_prob = probs[t_mask].cpu().numpy()\n",
        "\n",
        "    precisions, recalls, thresholds = precision_recall_curve(\n",
        "        y_train_true, y_train_prob\n",
        "    )\n",
        "\n",
        "    best_thresh = thresholds[np.argmax(f1s)]\n",
        "\n",
        "    y_pred_new = (y_test_prob >= best_thresh).astype(int)\n",
        "\n",
        "    print(f\"\\nBest Threshold: {best_thresh:.4f}\")\n",
        "    print(\"\\n===== FINAL RESULT (SAGE + NON-LOCAL TUNED) =====\")\n",
        "    print(classification_report(y_test_true, y_pred_new, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvF6XGlKeDp-",
        "outputId": "a219ddce-0c9d-48a3-84c1-0ae19a99530a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng 8 tổ hợp\n",
            "   Update: Hd=128, Lr=0.001, Dp=0.3 -> Train F1=0.7935\n",
            "   Update: Hd=128, Lr=0.003, Dp=0.3 -> Train F1=0.8277\n",
            "\n",
            " BEST NON-LOCAL PARAMS: (128, 0.003, 0.3)\n",
            "\n",
            "Best Threshold: 0.3867\n",
            "\n",
            "===== FINAL RESULT (SAGE + NON-LOCAL TUNED) =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9065    0.9053    0.9059       750\n",
            "           1     0.7593    0.7619    0.7606       294\n",
            "\n",
            "    accuracy                         0.8649      1044\n",
            "   macro avg     0.8329    0.8336    0.8333      1044\n",
            "weighted avg     0.8651    0.8649    0.8650      1044\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hd, lr, dp = 128, 0.003, 0.3\n",
        "best_thresh_test = 0.3867\n",
        "\n",
        "model = SAGE_With_NonLocal(in_dim=44, hidden_dim=hd, out_dim=2, dropout=dp).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(251):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.cross_entropy(out[t_mask], data.y[t_mask], weight=class_weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"   Epoch {epoch}: Loss {loss.item():.4f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_train_true = data.y[t_mask].cpu().numpy()\n",
        "    y_train_prob = probs[t_mask].cpu().numpy()\n",
        "\n",
        "    y_train_pred = (y_train_prob >= best_thresh_test).astype(int)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"===== TRAIN PERFORMANCE  =====\")\n",
        "    print(f\"Threshold: {best_thresh_test}\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_train_true, y_train_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4iZc1gUg0xo",
        "outputId": "4135b71b-865b-4a84-fa66-85e9af17d970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epoch 0: Loss 0.7781\n",
            "   Epoch 50: Loss 0.4271\n",
            "   Epoch 100: Loss 0.3641\n",
            "   Epoch 150: Loss 0.3129\n",
            "   Epoch 200: Loss 0.2591\n",
            "   Epoch 250: Loss 0.2244\n",
            "\n",
            "============================================================\n",
            "===== TRAIN PERFORMANCE  =====\n",
            "Threshold: 0.3867\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9913    0.8705    0.9270      8777\n",
            "           1     0.7105    0.9765    0.8225      2857\n",
            "\n",
            "    accuracy                         0.8965     11634\n",
            "   macro avg     0.8509    0.9235    0.8747     11634\n",
            "weighted avg     0.9223    0.8965    0.9013     11634\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Gating"
      ],
      "metadata": {
        "id": "yJV9tn42jrru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=4, dropout=0.2):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=in_channels,\n",
        "                                               num_heads=num_heads,\n",
        "                                               dropout=dropout,\n",
        "                                               batch_first=True)\n",
        "        self.norm = nn.LayerNorm(in_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x.unsqueeze(0)\n",
        "        attn_out, _ = self.attention(x_in, x_in, x_in)\n",
        "        x = x + self.dropout(attn_out.squeeze(0))\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class Gated_SAGE_NonLocal(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "\n",
        "        self.non_local = NonLocalBlock(in_channels=hidden_dim, num_heads=4, dropout=dropout)\n",
        "\n",
        "        self.gate_layer = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "        self.classifier = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        h_local = self.conv1(x, edge_index)\n",
        "        h_local = F.relu(h_local)\n",
        "        h_local = F.dropout(h_local, p=self.dropout, training=self.training)\n",
        "\n",
        "        h_global = self.non_local(h_local)\n",
        "\n",
        "        concat = torch.cat([h_local, h_global], dim=1)\n",
        "        z = torch.sigmoid(self.gate_layer(concat))\n",
        "\n",
        "        h_fused = (1 - z) * h_local + z * h_global\n",
        "\n",
        "        out = self.conv2(h_fused, edge_index)\n",
        "        out = F.relu(out)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "1RoPYib2iK_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "hd, lr, dp = 128, 0.003, 0.3\n",
        "\n",
        "print(f\"GATED SAGE + Non-Local\")\n",
        "print(f\"    Params: Hidden={hd}, LR={lr}, Dropout={dp}\")\n",
        "\n",
        "model = Gated_SAGE_NonLocal(in_dim=44, hidden_dim=hd, out_dim=2, dropout=dp).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "# Loss weights\n",
        "weights = compute_class_weight('balanced', classes=np.array([0,1]), y=data.y[t_mask].cpu().numpy())\n",
        "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
        "\n",
        "# --- TRAIN LOOP ---\n",
        "model.train()\n",
        "for epoch in range(250):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.cross_entropy(out[t_mask], data.y[t_mask], weight=class_weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"   Epoch {epoch}: Loss {loss.item():.4f}\")\n",
        "\n",
        "# --- EVALUATE ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_train_true = data.y[t_mask].cpu().numpy()\n",
        "    y_train_prob = probs[t_mask].cpu().numpy()\n",
        "\n",
        "    precisions, recalls, thresholds = precision_recall_curve(\n",
        "        y_train_true, y_train_prob\n",
        "    )\n",
        "\n",
        "    best_thresh = thresholds[np.argmax(\n",
        "        2*(precisions*recalls)/(precisions+recalls+1e-10)\n",
        "    )]\n",
        "\n",
        "    y_pred_new = (y_test_prob >= best_thresh).astype(int)\n",
        "\n",
        "    print(f\"\\nBest Threshold: {best_thresh:.4f}\")\n",
        "    print(\"\\n===== FINAL RESULT: GATED SAGE + NON-LOCAL =====\")\n",
        "    print(classification_report(y_test_true, y_pred_new, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSGw1Ix0iR3X",
        "outputId": "969b2e5e-d7ae-46ff-86ad-3eef3754cf4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GATED SAGE + Non-Local...\n",
            "    Params: Hidden=128, LR=0.003, Dropout=0.3\n",
            "   Epoch 0: Loss 0.7018\n",
            "   Epoch 50: Loss 0.4233\n",
            "   Epoch 100: Loss 0.3600\n",
            "   Epoch 150: Loss 0.3101\n",
            "   Epoch 200: Loss 0.2617\n",
            "\n",
            "Best Threshold: 0.6934\n",
            "\n",
            "===== FINAL RESULT: GATED SAGE + NON-LOCAL =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8839    0.9640    0.9222       750\n",
            "           1     0.8805    0.6769    0.7654       294\n",
            "\n",
            "    accuracy                         0.8831      1044\n",
            "   macro avg     0.8822    0.8204    0.8438      1044\n",
            "weighted avg     0.8829    0.8831    0.8780      1044\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(data)\n",
        "    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "    y_train_true = data.y[t_mask].cpu().numpy()\n",
        "    y_train_prob = probs[t_mask].cpu().numpy()\n",
        "\n",
        "    applied_thresh = 0.6934\n",
        "\n",
        "    y_train_pred = (y_train_prob >= applied_thresh).astype(int)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"===== TRAIN PERFORMANCE (Applied Threshold: {applied_thresh:.4f}) =====\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_train_true, y_train_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjzCviwhj1sh",
        "outputId": "0101b1f0-269a-4721-f138-5a440084b14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "===== TRAIN PERFORMANCE (Applied Threshold: 0.6934) =====\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9525    0.9708    0.9616      8777\n",
            "           1     0.9048    0.8512    0.8772      2857\n",
            "\n",
            "    accuracy                         0.9415     11634\n",
            "   macro avg     0.9286    0.9110    0.9194     11634\n",
            "weighted avg     0.9408    0.9415    0.9409     11634\n",
            "\n"
          ]
        }
      ]
    }
  ]
}